{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"#crisp-compliant-ros2-controllers-for-learning-based-manipulation-policies","title":"CRISP - Compliant ROS2 Controllers for Learning-Based Manipulation Policies","text":"<p>Authors: Daniel San Jose Pro, Oliver Hausd\u00f6rfer, Ralf R\u00f6mer, Maximilian D\u00f6sch Martin Schuck and Angela Sch\u00f6llig.</p> <p>Collection of C++ controllers for torque-based control for manipulators compatible with <code>ros2_control</code>, including Operational Space Control and Cartesian Impedance Control. Developed for deploying high-level learning-based policies (VLA, Diffusion, ...).</p> <p>If you use this work, please cite it using below bibtex.</p> <p>Check the controllers (CRISP controllers)  , robot demos (CRISP controllers demos) , a simple python interface (CRISP_PY) , and a gym wrapper (CRISP_GYM)  for real-world experiments.</p> Robot teleoperated using a Follower-Leader system in CRISP GYM  Diffusion Policy trained and deployed from the same demonstrations. Robot following a moving target, while base joint follows a sine curve. Simulated kinova robot with continous joints and nullspace control Simulated iiwa robot example... Real robot following a target and being disturbed (contact) + null space control demonstration Demonstration using a cartesian controller teleoperated using Vicon tracking system (Speed x4) Teleoperation setup with feedback wrench available."},{"location":"#why","title":"Why?","text":"<p>Learning-based controllers, such as diffusion policies, deep reinforcement learning, and vision-action-models in general, typically output low-frequency or sporadic target poses, necessitating a low-level controller to track these references smoothly, especially in contact-rich environments. While <code>ROS2</code> frameworks like <code>MoveIt</code> offer comprehensive motion planning capabilities, they are often unnecessarily complex for tasks requiring simple, real-time pose or joint servoing.</p> <p>We present a set of lightweight, torque-based Cartesian and joint-space controllers implemented in C++ for <code>ros2_control</code>, compatible with any robot exposing an effort interface\u2014a common standard among modern manipulators. Our controllers incorporate friction compensation, joint limit avoidance, and error clipping, and have been validated on the Franka Robotics FR3 manipulator.</p> <p>Designed for fast integration and real-time control, our implementation lowers the barrier to deploying learning-based algorithms on <code>ROS2</code>-compatible platforms.</p>"},{"location":"#features","title":"Features","text":"<ul> <li>\ud83e\udd16 Operational Space Controller as well as Cartesian Impedance Controller for torque-based control.  </li> <li>\ud83d\udeab No MoveIt or complicated path-planning, just a simple C++ <code>ros2_controller</code>. Ready to use.  </li> <li>\u2699\ufe0f Dynamically and highly parametrizable: powered by the <code>generate_parameter_library</code> you can modify stiffness and more during operation.  </li> <li>\ud83d\udc0d Python interface to move your ROS2 robot around without having to think about topics, spinning, and more ROS2 concepts but without loosing the powerful ROS2 API. Check crisp_py for more information and examples.</li> <li>\ud83d\udd01 Gym environment with utilities to record trajectories in LeRobotFormat and deploy trained policies. Check crisp_gym.</li> <li>\u2753 Demos showcasing how to use the controller with FR3 of Franka Emika in single and bimanual setup. Check the crisp_controller_demos.</li> </ul>"},{"location":"#citing","title":"Citing","text":"<pre><code>@inproceeding{\n   TODO\n}\n</code></pre>"},{"location":"controllers/","title":"Available controllers","text":""},{"location":"controllers/#default-controllers","title":"Default controllers","text":"<p>In CRISP_PY you can find some of the default configurations that we used with the FR3 of Franka Robotics. These configurations are:</p> <ul> <li><code>default_cartesian_impedance.yaml</code>: A simple config for a Cartesian Impedance (CI) Controller. This is our go-to controller, since it offers nice contact behavior and is the perfect balance between compliance and precision.</li> <li><code>default_operational_space_controller.yaml</code>: A simple config for Operational Space Controller (OSC). Is a slightly stiffer controller. The contact behavior might not be as nice as for CI.</li> <li><code>clipped_cartesian_impedance.yaml</code>: A simple config of a highly clipped error CI (CI-clipped). This offers a highly stiff controller but at the same time safe since the error is highly clipped. Useful for precise manipulation.</li> </ul> <p>Experiments using these sets of parameters can be found in the paper of the framework.</p>"},{"location":"controllers/#_1","title":"","text":""},{"location":"getting_started/","title":"Getting started","text":"<p>Here is an overview of the framework.</p> <p> </p> <p>In short, you have:</p> <ul> <li> 1. The first part is the setup for the low-level controllers, i.e., crisp_controllers.</li> <li> 2. Then, we will try to move the robot around using crisp_py.</li> <li> 3. After that, we can include cameras in the setup or further sensors. </li> <li> 4. Finally, we can set up crisp_gym and start recording episodes.</li> </ul>"},{"location":"getting_started/#1-getting-the-low-level-controller-ready","title":"1. Getting the low-level controller ready","text":"<p>First, the computer running the controller needs a real-time patch for the controller to run smoothly and safely. You can check out the Franka Robotics guide on how to set up a RT-patch. On newer Ubuntu versions, you can use Ubuntu Pro for an easy setup.</p> <p>Then, check if your robot is part of our demos. You can then follow the instructions there to start your robot(s) using a Docker container. Some of them offer the possibility to run the demos with simulated robots to test the setup.</p> <p>If your robot is not included in the demos, check out How to set up a robot that is not available in the demos. If you get the controllers running, feel free to open a pull request to add it to the demos!</p>"},{"location":"getting_started/#2-use-crisp_py-to-control-the-robot","title":"2. Use crisp_py to control the robot","text":""},{"location":"getting_started/#installation","title":"Installation","text":"<p>To use <code>crisp_py</code>, we recommend using pixi as a package manager, a modern conda-like package manager. It can be used in combination with robostack to easily install ROS2 in any machine. There are a few ways to get you started:</p> <p>... install from source:</p> <pre><code>git clone https://github.com/utiasDSL/crisp_py\npixi install\npixi shell -e humble\npython -c \"import crisp_py\"  # (1)!\n</code></pre> <ol> <li>This should not log anything if everything is fine</li> </ol> <p>... use in your already existing pixi project:</p> <p>To use <code>crisp_py</code> in an already existing pixi project, you need to make sure that <code>ROS2</code> is available. Check the pixi.toml of <code>crisp_py</code> to see how this looks like. Then you can add <code>crisp_py</code> as a pypi package: <pre><code>pixi add --pypi crisp-py\n</code></pre> or <pre><code>uv add crisp-py\n</code></pre> or <pre><code>pip install crisp-py\n</code></pre> Double-check that everything is working by running:</p> <pre><code>python -c \"import crisp_py\"  # (1)!\n</code></pre> <ol> <li>This should not log anything if everything is fine</li> </ol> <p>Now you can try to control the robot! Check out the examples for inspiration.</p>"},{"location":"getting_started/#try-it-out-with-the-robot","title":"Try it out with the robot","text":"<p>Make sure that the demo container is running in the background, as we will need it to access the robot. From now on, you can instantiate <code>Robot</code> objects to control the robot.</p> Example robot usage: <pre><code>from crisp_py.robot import Robot\nfrom crisp_py.robot_config import RobotConfig\n\nrobot_config = RobotConfig(...)\nrobot = Robot(namespace=\"...\", config=robot_config)  # (1)!\nrobot.wait_until_ready()  # (2)!\n\nprint(robot.end_effector_pose)\n\n\nrobot.controller_switcher_client.switch_controller(\n    \"cartesian_impedance_controller\",  # (4)!\n)  \nx, y, z = robot.end_effector_pose.position\nrobot.set_target(position=[x, y, z-0.1])  # (3)!\n\nrobot.shutdown()\n</code></pre> <ol> <li>This will get information from the robot asynchronously</li> <li>Make sure that we get information from the robot before trying to set targets or reading the pose of the robot.</li> <li>Set target 10 cm downwards. Careful not to send poses that are too far away from the current one!</li> <li>This will request the controller manager to activate the cartesian impedance controller. You can use it with other controllers like the operational space controller!</li> </ol>"},{"location":"getting_started/#3-adding-cameras-grippers-and-further-sensors","title":"3. Adding cameras, grippers, and further sensors","text":""},{"location":"getting_started/#cameras","title":"Cameras","text":"<p>To add a camera, you will need to run it in a separate container as well. The cameras that we tried are:</p> <ul> <li>Real Sense which gives amazing ROS2 support,</li> <li>or Orbbec.</li> </ul> <p>But any camera should work with camera_ros.</p> Example camera usage: <pre><code>import cv2\nfrom crisp_py.camera import Camera, CameraConfig\n\ncamera_config = CameraConfig(\n    camera_name=\"primary\",\n    resolution=(256, 256),  # (1)!\n    camera_color_image_topic=\"camera_name/color/image_raw\",  # (2)!\n    camera_color_info_topic=\"camera_name/color/camera_info\",\n)\n\ncamera = Camera(config=camera_config)  # (3)!\ncamera.wait_until_ready() # (4)!\n\ncv2.imshow(\"Camera Image\", camera.current_image)  # (5)!\ncv2.waitKey(0)\n</code></pre> <ol> <li>You can define a custom resolution, independently of the resolution of the published image.</li> <li>Set here the topic of your custom camera name</li> <li>You can also pass <code>namespace=\"...\"</code> to give the camera a namespace. This is required for a bimanual setup.</li> <li>Make sure that we received an image. This will fail with a timeout if the topic is wrong or the camera is not publishing.</li> <li>This will show you the latest received image!</li> </ol>"},{"location":"getting_started/#grippers","title":"Grippers","text":"<p>For gripper control, you need to make sure that a ROS2 node is running that accepts commands through a topic and publishes the state of the gripper. To use a:</p> <ul> <li>Franka Hand, you just need to start the demo. An adapter is already running to allow you to control the gripper this way,</li> <li>Dynamixel motor to control a gripper, we used the well-maintained dynamixel_hardware_interface with a position controller for the gripper.</li> </ul> Example gripper usage: <p>You can use the gripper in <code>crisp_py</code> with: <pre><code>from crisp_py.gripper import Gripper, GripperConfig\n\n# config = GripperConfig.from_yaml(path=\"...\")  (1)\nconfig = GripperConfig(\n    min_value=0.0,\n    max_value=1.0,\n    command_topic=\"gripper_position_controller/commands\",\n    joint_state_topic=\"joint_states\",\n)  # (2)!\ngripper = Gripper(gripper_config=config)  # (3)!\ngripper.wait_until_ready()  # (4)!\n\nprint(gripper.value)\n\ngripper.open()\n# gripper.close()\n# gripper.set_target(0.5)\n</code></pre></p> <ol> <li>You can load the configs from a yaml file. If you calibrate the gripper manually (check the crisp_py docs for more information) you can select this way your custom calibration file.</li> <li>Set the range of allowed commands (min stands for fully closed, max to fully open) and the topics for the gripper. You can check the topics using <code>ros2 topic list</code></li> <li>You can also pass <code>namespace=\"...\"</code> to give the gripper a namespace. This is required for a bimanual setup.</li> <li>Make sure that we received a gripper value. This will fail with a timeout if the topic is wrong or the gripper is not publishing.</li> </ol>"},{"location":"getting_started/#sensors","title":"Sensors","text":"<p>You can add further sensors (Force Torque Sensor, Tactile Sensor...) by adding a custom <code>Sensor</code> that subscribes to a topic. Check the examples for more information.</p>"},{"location":"getting_started/#4-using-the-gym","title":"4. Using the Gym","text":"<p>Similar to <code>crisp_py</code>, we recommend using <code>pixi</code> to install <code>crisp_gym</code>.</p> <pre><code>git clone https://github.com/utiasDSL/crisp_gym\npixi install\npixi shell -e humble\npython -c \"import crisp_gym\"\n</code></pre>"},{"location":"getting_started/#record-data-in-lerobotformat","title":"Record data in LeRobotFormat","text":"<p>You can record data in <code>LeRobotFormat</code> to train a policy directly in lerobot by running:</p> <p>TODO: Add installation steps for lerobot.</p> <pre><code>pixi run -e humble-lerobot python scripts/record_data.py\n</code></pre> <p>After recording the data, you can use the dataset to train a policy with lerobot. They provide the latest implementations.</p>"},{"location":"getting_started/#deploy-policy","title":"Deploy policy","text":"<p>...with <code>lerobot</code>: <pre><code>pixi run -e humble-lerobot python scripts/deploy_policy.py\n</code></pre></p>"},{"location":"new_robot_setup/","title":"Setting up a robot that is not available in the demos","text":"<p>For ROS2 users, adding our controllers to their stack should be straightforward as it works like any other ROS2 controller. For novice ROS2 users, we prepared a more detailed guide to make sure that you get your robot ready:</p> <ol> <li> <p>First make sure that your ROS2 drivers for the robot you want to use allow direct-torque control i.e. the effort command interface is available. Usually, you should be able to find it in the Github repository.  If this is the case, you are good to go.  Otherwise, you might need to use cartesian_controllers for position controlled robots.</p> </li> <li> <p>Install the ROS2 drivers on your computer.  We recommend to work in a devcontainer to avoid installing ROS2 directly in your machine. You can check the demos for inspiration.</p> </li> <li> <p>Add the controllers to your src folder where the ROS2 drivers for your robot have been installed:     <pre><code>cd ~/ros2_ws  # or wherever you ws is...\ngit clone https://github.com/utiasDSL/crisp_controllers.git src/crisp_controllers\nsource /opt/ros/$ROS_DISTRO/setup.sh\nsource /install/setup.sh\nrosdep update\nrosdep install -q --from-paths src --ignore-src -y  # (1)!\ncolcon build --packages-select crisp_controllers \ntouch src/crisp_controllers/COLCON_IGNORE  # (2)! \nsource /install/setup.sh  # (3)!\n</code></pre></p> <ol> <li>This line will make sure that all missing dependencies are installed</li> <li>By adding this file, the controller will not be built over and over</li> <li>Don't forget to source again to make sure that ROS2 can find the new package</li> </ol> </li> <li> <p>Now, you will need to add the controller to the config file of the controller_manager. Usually, you will find this config file in the bringup package where the launch files are located (called <code>&lt;robotname&gt;_bringup</code>) and are saved as <code>controllers.yaml</code>. Check out the FR3 config to get an idea how the config file looks like. For more information on the controllers, check the available controllers and broadcaster page.</p> How to add the configuration to the config file <pre><code>/**:\ncontroller_manager:\n    ros__parameters:\n    update_rate: 1000  # Hz\n\n    pose_broadcaster:\n        type: crisp_controllers/PoseBroadcaster\n\n    gravity_compensation:\n        type: crisp_controllers/CartesianImpedanceController\n\n    cartesian_impedance_controller:\n        type: crisp_controllers/CartesianImpedanceController\n\n    joint_impedance_controller:\n        type: crisp_controllers/CartesianImpedanceController\n\n    # more controllers...\n/**:\npose_broadcaster:\n    ros__parameters:\n    joints:\n        - &lt;TODO&gt;\n\n    end_effector_frame: &lt;TODO&gt;\n    base_frame: base\n\ngravity_compensation:\n    ros__parameters:\n    joints:\n        &lt;TODO&gt;\n\n    end_effector_frame: &lt;TODO&gt;\n    base_frame: base\n\n    task:\n        k_pos_x: 0.0\n        k_pos_y: 0.0\n        k_pos_z: 0.0\n        k_rot_x: 30.0\n        k_rot_y: 30.0\n        k_rot_z: 30.0\n\n    nullspace: \n        stiffness: 0.0\n\n    use_friction: true\n    use_coriolis_compensation: true\n    use_local_jacobian: true\n\njoint_impedance_controller:\n    ros__parameters:\n    joints:\n        &lt;TODO&gt;\n\n    end_effector_frame: &lt;TODO&gt;\n    base_frame: base\n\n    task:\n        k_pos_x: 0.0\n        k_pos_y: 0.0\n        k_pos_z: 0.0\n        k_rot_x: 0.0\n        k_rot_y: 0.0\n        k_rot_z: 0.0\n\n    max_delta_tau: 0.5\n\n    nullspace: \n        stiffness: 5.0\n        projector_type: none  # So we are directly controlling the joints!\n        damping: 0.5\n        max_tau: 5.0\n        regularization: 1.0e-06\n        weights:\n        &lt;TODO&gt;\n\n    use_friction: true\n    use_coriolis_compensation: true\n    use_local_jacobian: true\n    limit_error: true\n    limit_torques: true\n\n\ncartesian_impedance_controller:\n    ros__parameters:\n    joints:\n        - &lt;TODO&gt;\n\n    end_effector_frame: &lt;TODO&gt;\n    base_frame: &lt;TODO&gt;\n\n    task:\n        k_pos_x: 400.0\n        k_pos_y: 400.0\n        k_pos_z: 400.0\n        k_rot_x: 30.0\n        k_rot_y: 30.0\n        k_rot_z: 30.0\n\n    nullspace: \n        stiffness: 2.0\n\n    use_friction: false\n    use_coriolis_compensation: true\n    use_local_jacobian: true\n</code></pre> </li> <li> <p>Finally add them to your launch file.      You will need to use the controller_manager spawner to start the controllers at launch.     Usually, in the launch file you will find other controllers/broadcasters that are being launched.     Just duplicate the nodes and pass the correct names to activate the controllers.     Check out FR3 launch file for inspiration.</p> How to add the controllers to the launch file <p>Add the following nodes to the launch description:</p> <pre><code>...\nNode(\n    package=\"controller_manager\",\n    executable=\"spawner\",\n    arguments=[\"cartesian_impedance_controller\", \"--inactive\"],\n    output=\"screen\",\n),\nNode(\n    package=\"controller_manager\",\n    executable=\"spawner\",\n    arguments=[\"joint_impedance_controller\", \"--inactive\"],\n    output=\"screen\",\n),\nNode(\n    package=\"controller_manager\",\n    executable=\"spawner\",\n    arguments=[\"gravity_compensation\", \"--inactive\"],\n    output=\"screen\",\n),\nNode(\n    package=\"controller_manager\",\n    executable=\"spawner\",\n    arguments=[\"pose_broadcaster\"],\n    output=\"screen\",\n),\n...\n</code></pre> </li> <li> <p>Voila! After launching your robot you should see that new controller are being loaded. If you get stuck somewhere in the process feel free to open an issue.</p> </li> </ol>"}]}